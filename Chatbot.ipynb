{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROD9s3R0YHhS",
        "outputId": "fbf5a563-5d52-4d92-e81e-5fa909702ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded data from /content/Online Retail.xlsx\n",
            "Shape: (541909, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training question classifier...\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " correlation       1.00      1.00      1.00         3\n",
            "       count       0.00      0.00      0.00         1\n",
            "distribution       1.00      1.00      1.00         1\n",
            "      filter       0.67      1.00      0.80         2\n",
            "         max       0.67      1.00      0.80         2\n",
            "        mean       1.00      1.00      1.00         3\n",
            "         min       1.00      0.50      0.67         2\n",
            "         sum       1.00      1.00      1.00         1\n",
            "         top       1.00      1.00      1.00         2\n",
            "       trend       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.89        18\n",
            "   macro avg       0.83      0.85      0.83        18\n",
            "weighted avg       0.87      0.89      0.86        18\n",
            "\n",
            "\n",
            "===== Excel Question Answering System =====\n",
            "Loaded file: /content/Online Retail.xlsx\n",
            "Dataset has 541909 rows and 8 columns\n",
            "Columns: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country\n",
            "\n",
            "Ask questions about your data (type 'exit' to quit):\n",
            "\n",
            "Your question: give mean of quantity\n",
            "\n",
            "Answer:\n",
            "There are 541909 rows in the dataset.\n",
            "\n",
            "Your question: whats the mean quantity\n",
            "\n",
            "Answer:\n",
            "There are 541909 rows in the dataset.\n",
            "\n",
            "Your question: sum of quantity\n",
            "\n",
            "Answer:\n",
            "The sum of 'Quantity' is 5176450.\n",
            "\n",
            "Your question: average of unit price\n",
            "\n",
            "Answer:\n",
            "The average/mean value of 'Quantity' is 9.55.\n",
            "\n",
            "Your question: average of UnitPrice\n",
            "\n",
            "Answer:\n",
            "The average/mean value of 'UnitPrice' is 4.61.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "class ExcelQuestionAnswerer:\n",
        "    def __init__(self, file_path):\n",
        "        \"\"\"\n",
        "        Initialize the question answerer with an Excel file.\n",
        "\n",
        "        Args:\n",
        "        f   ile_path (str): Path to the Excel file\n",
        "        \"\"\"\n",
        "        self.file_path = file_path\n",
        "        self.df = None\n",
        "        self.column_descriptions = {}\n",
        "        self.data_summary = {}\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.question_classifier = None\n",
        "\n",
        "        # Load data and continue only if successful\n",
        "        if self.load_data():\n",
        "            self.analyze_data()\n",
        "            self.generate_qa_templates()\n",
        "        else:\n",
        "            print(\"Failed to initialize the Excel Question Answerer. Please check the file path.\")\n",
        "            # Initialize empty structures to prevent errors\n",
        "            self.data_summary = {\n",
        "                'total_rows': 0,\n",
        "                'total_columns': 0,\n",
        "                'column_names': [],\n",
        "                'column_types': {}\n",
        "            }\n",
        "            self.qa_templates = {}\n",
        "            self.example_questions = []\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load the Excel file into a pandas DataFrame.\"\"\"\n",
        "        if not os.path.exists(self.file_path):\n",
        "            print(f\"Excel file not found: {self.file_path}\")\n",
        "            print(\"Please ensure the file exists at the specified path.\")\n",
        "            self.df = None\n",
        "            return False  # Return False instead of exiting\n",
        "\n",
        "        try:\n",
        "            self.df = pd.read_excel(self.file_path)\n",
        "            print(f\"Successfully loaded data from {self.file_path}\")\n",
        "            print(f\"Shape: {self.df.shape}\")\n",
        "            return True  # Return True on success\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading Excel file: {e}\")\n",
        "            self.df = None\n",
        "            return False  # Return False on error\n",
        "\n",
        "    def analyze_data(self):\n",
        "        \"\"\"Analyze the data to generate insights.\"\"\"\n",
        "        # Check if DataFrame is loaded properly\n",
        "        if self.df is None:\n",
        "            print(\"No data to analyze. Please check if the Excel file was loaded correctly.\")\n",
        "            # Initialize empty dictionaries to prevent further errors\n",
        "            self.data_summary = {\n",
        "                'total_rows': 0,\n",
        "                'total_columns': 0,\n",
        "                'column_names': [],\n",
        "                'column_types': {}\n",
        "            }\n",
        "            return\n",
        "        self.data_summary['total_rows'] = len(self.df)\n",
        "        self.data_summary['total_columns'] = len(self.df.columns)\n",
        "        self.data_summary['column_names'] = list(self.df.columns)\n",
        "        self.data_summary['column_types'] = self.df.dtypes.to_dict()\n",
        "\n",
        "        # For each column, gather descriptive statistics\n",
        "        for column in self.df.columns:\n",
        "            self.column_descriptions[column] = {}\n",
        "\n",
        "            # Get column data type\n",
        "            col_type = self.df[column].dtype\n",
        "\n",
        "            if pd.api.types.is_numeric_dtype(col_type):\n",
        "                # Numeric column\n",
        "                self.column_descriptions[column]['type'] = 'numeric'\n",
        "                self.column_descriptions[column]['min'] = self.df[column].min()\n",
        "                self.column_descriptions[column]['max'] = self.df[column].max()\n",
        "                self.column_descriptions[column]['mean'] = self.df[column].mean()\n",
        "                self.column_descriptions[column]['median'] = self.df[column].median()\n",
        "                self.column_descriptions[column]['std'] = self.df[column].std()\n",
        "                self.column_descriptions[column]['missing'] = self.df[column].isna().sum()\n",
        "\n",
        "            elif pd.api.types.is_datetime64_dtype(col_type):\n",
        "                # Datetime column\n",
        "                self.column_descriptions[column]['type'] = 'datetime'\n",
        "                self.column_descriptions[column]['min'] = self.df[column].min()\n",
        "                self.column_descriptions[column]['max'] = self.df[column].max()\n",
        "                self.column_descriptions[column]['range_days'] = (self.df[column].max() - self.df[column].min()).days\n",
        "                self.column_descriptions[column]['missing'] = self.df[column].isna().sum()\n",
        "\n",
        "            elif pd.api.types.is_string_dtype(col_type) or pd.api.types.is_object_dtype(col_type):\n",
        "                # String/categorical column\n",
        "                self.column_descriptions[column]['type'] = 'categorical'\n",
        "                self.column_descriptions[column]['unique_values'] = self.df[column].nunique()\n",
        "                self.column_descriptions[column]['most_common'] = self.df[column].value_counts().nlargest(5).to_dict()\n",
        "                self.column_descriptions[column]['missing'] = self.df[column].isna().sum()\n",
        "\n",
        "            # Try to infer column meaning from name\n",
        "            if 'date' in column.lower() or 'time' in column.lower():\n",
        "                self.column_descriptions[column]['semantic_type'] = 'date/time'\n",
        "            elif 'price' in column.lower() or 'cost' in column.lower() or 'revenue' in column.lower():\n",
        "                self.column_descriptions[column]['semantic_type'] = 'monetary'\n",
        "            elif 'id' in column.lower() or 'key' in column.lower():\n",
        "                self.column_descriptions[column]['semantic_type'] = 'identifier'\n",
        "            elif 'name' in column.lower():\n",
        "                self.column_descriptions[column]['semantic_type'] = 'name'\n",
        "            elif 'qty' in column.lower() or 'quantity' in column.lower() or 'count' in column.lower():\n",
        "                self.column_descriptions[column]['semantic_type'] = 'quantity'\n",
        "            else:\n",
        "                self.column_descriptions[column]['semantic_type'] = 'unknown'\n",
        "\n",
        "        # Generate correlations for numeric columns\n",
        "        numeric_columns = self.df.select_dtypes(include=['number']).columns\n",
        "        if len(numeric_columns) >= 2:\n",
        "            self.data_summary['correlations'] = self.df[numeric_columns].corr().to_dict()\n",
        "\n",
        "    def generate_qa_templates(self):\n",
        "        \"\"\"Generate question and answer templates based on the data structure.\"\"\"\n",
        "        self.qa_templates = {\n",
        "            \"count\": \"How many {rows/column} are in the {dataset/column}?\",\n",
        "            \"min\": \"What is the minimum value of {column}?\",\n",
        "            \"max\": \"What is the maximum value of {column}?\",\n",
        "            \"mean\": \"What is the average/mean value of {column}?\",\n",
        "            \"sum\": \"What is the total/sum of {column}?\",\n",
        "            \"unique\": \"What are the unique values in {column}?\",\n",
        "            \"missing\": \"How many missing values are there in {column}?\",\n",
        "            \"correlation\": \"Is there a correlation between {column1} and {column2}?\",\n",
        "            \"filter\": \"Show me all rows where {column} {condition}\",\n",
        "            \"group\": \"What is the {aggregation} of {column} grouped by {group_column}?\",\n",
        "            \"top\": \"What are the top {N} values in {column}?\",\n",
        "            \"bottom\": \"What are the bottom {N} values in {column}?\",\n",
        "            \"distribution\": \"What is the distribution of {column}?\",\n",
        "            \"trend\": \"What is the trend of {column} over {time_column}?\"\n",
        "        }\n",
        "\n",
        "        # Generate example questions for training\n",
        "        self.example_questions = self.generate_example_questions()\n",
        "    def interactive_qa(self):\n",
        "        \"\"\"Start an interactive Q&A session with the user.\"\"\"\n",
        "        # Check if the data was loaded successfully\n",
        "        if self.df is None:\n",
        "           print(\"Cannot start interactive session: No data was loaded.\")\n",
        "           return\n",
        "\n",
        "        # Train the question classifier if not already trained\n",
        "        if self.question_classifier is None:\n",
        "            print(\"Training question classifier...\")\n",
        "            self.train_question_classifier()\n",
        "\n",
        "        print(\"\\n===== Excel Question Answering System =====\")\n",
        "        print(f\"Loaded file: {self.file_path}\")\n",
        "        print(f\"Dataset has {self.data_summary['total_rows']} rows and {self.data_summary['total_columns']} columns\")\n",
        "        print(\"Columns:\", \", \".join(self.data_summary['column_names']))\n",
        "        print(\"\\nAsk questions about your data (type 'exit' to quit):\")\n",
        "\n",
        "        while True:\n",
        "            question = input(\"\\nYour question: \")\n",
        "\n",
        "            if question.lower() in ['exit', 'quit', 'bye']:\n",
        "               print(\"Goodbye!\")\n",
        "               break\n",
        "\n",
        "            # Process the question and get the answer\n",
        "            answer = self.answer_question(question)\n",
        "\n",
        "            # Print the answer\n",
        "            print(\"\\nAnswer:\")\n",
        "            print(answer)\n",
        "\n",
        "    def generate_example_questions(self):\n",
        "        \"\"\"Generate example questions for each template using actual column names.\"\"\"\n",
        "        examples = []\n",
        "\n",
        "        # Get column names by type for appropriate substitution\n",
        "        numeric_cols = self.df.select_dtypes(include=['number']).columns.tolist()\n",
        "        categorical_cols = [col for col in self.df.columns if self.df[col].nunique() < 20 and not pd.api.types.is_numeric_dtype(self.df[col].dtype)]\n",
        "        datetime_cols = [col for col, desc in self.column_descriptions.items() if desc.get('semantic_type') == 'date/time']\n",
        "\n",
        "        # For each template, generate at least 3 example questions\n",
        "        if numeric_cols:\n",
        "            # Count questions\n",
        "            examples.append((\"How many rows are in the dataset?\", \"count\"))\n",
        "            examples.append((f\"How many records does this data have?\", \"count\"))\n",
        "            examples.append((f\"What's the total number of entries?\", \"count\"))\n",
        "\n",
        "            # Min questions\n",
        "            for col in numeric_cols[:3]:  # Use up to 3\n",
        "                examples.append((f\"What is the minimum value of {col}?\", \"min\"))\n",
        "                examples.append((f\"What's the lowest {col}?\", \"min\"))\n",
        "                examples.append((f\"What's the smallest value in the {col} column?\", \"min\"))\n",
        "\n",
        "            # Max questions\n",
        "            for col in numeric_cols[:3]:\n",
        "                examples.append((f\"What is the maximum value of {col}?\", \"max\"))\n",
        "                examples.append((f\"What's the highest {col}?\", \"max\"))\n",
        "                examples.append((f\"What's the largest value in the {col} column?\", \"max\"))\n",
        "\n",
        "            # Mean questions\n",
        "            for col in numeric_cols[:3]:\n",
        "                examples.append((f\"What is the average value of {col}?\", \"mean\"))\n",
        "                examples.append((f\"What's the mean {col}?\", \"mean\"))\n",
        "                examples.append((f\"Calculate the average {col}.\", \"mean\"))\n",
        "\n",
        "            # Sum questions\n",
        "            for col in numeric_cols[:3]:\n",
        "                examples.append((f\"What is the total of {col}?\", \"sum\"))\n",
        "                examples.append((f\"Sum all values in {col}.\", \"sum\"))\n",
        "                examples.append((f\"What's the sum of {col}?\", \"sum\"))\n",
        "\n",
        "            # Correlation questions\n",
        "            if len(numeric_cols) >= 2:\n",
        "                for i in range(min(3, len(numeric_cols))):\n",
        "                    for j in range(i+1, min(i+3, len(numeric_cols))):\n",
        "                        examples.append((f\"Is there a correlation between {numeric_cols[i]} and {numeric_cols[j]}?\", \"correlation\"))\n",
        "                        examples.append((f\"How do {numeric_cols[i]} and {numeric_cols[j]} relate?\", \"correlation\"))\n",
        "                        examples.append((f\"Does {numeric_cols[i]} correlate with {numeric_cols[j]}?\", \"correlation\"))\n",
        "\n",
        "            # Filter questions\n",
        "            for col in numeric_cols[:3]:\n",
        "                examples.append((f\"Show me all rows where {col} > {self.df[col].mean():.1f}\", \"filter\"))\n",
        "                examples.append((f\"Filter data where {col} is less than {self.df[col].min() + (self.df[col].max() - self.df[col].min())/4:.1f}\", \"filter\"))\n",
        "                examples.append((f\"Which records have {col} equal to {self.df[col].median():.1f}?\", \"filter\"))\n",
        "\n",
        "            # Top/Bottom questions\n",
        "            for col in numeric_cols[:3]:\n",
        "                examples.append((f\"What are the top 5 values in {col}?\", \"top\"))\n",
        "                examples.append((f\"Show me the 10 highest {col}.\", \"top\"))\n",
        "                examples.append((f\"What are the 3 largest values of {col}?\", \"top\"))\n",
        "                examples.append((f\"What are the bottom 5 values in {col}?\", \"bottom\"))\n",
        "                examples.append((f\"Show me the 10 lowest {col}.\", \"bottom\"))\n",
        "                examples.append((f\"What are the 3 smallest values of {col}?\", \"bottom\"))\n",
        "\n",
        "            # Distribution questions\n",
        "            for col in numeric_cols[:3]:\n",
        "                examples.append((f\"What is the distribution of {col}?\", \"distribution\"))\n",
        "                examples.append((f\"Show me the spread of {col}.\", \"distribution\"))\n",
        "                examples.append((f\"How is {col} distributed?\", \"distribution\"))\n",
        "\n",
        "        if categorical_cols:\n",
        "            # Unique questions\n",
        "            for col in categorical_cols[:3]:\n",
        "                examples.append((f\"What are the unique values in {col}?\", \"unique\"))\n",
        "                examples.append((f\"What distinct values does {col} have?\", \"unique\"))\n",
        "                examples.append((f\"List all possible values of {col}.\", \"unique\"))\n",
        "\n",
        "            # Group questions\n",
        "            if numeric_cols and categorical_cols:\n",
        "                for cat_col in categorical_cols[:2]:\n",
        "                    for num_col in numeric_cols[:2]:\n",
        "                        examples.append((f\"What is the average of {num_col} grouped by {cat_col}?\", \"group\"))\n",
        "                        examples.append((f\"Show me the sum of {num_col} for each {cat_col}.\", \"group\"))\n",
        "                        examples.append((f\"Calculate the total {num_col} by {cat_col}.\", \"group\"))\n",
        "\n",
        "        if datetime_cols and numeric_cols:\n",
        "            # Trend questions\n",
        "            for time_col in datetime_cols[:2]:\n",
        "                for value_col in numeric_cols[:2]:\n",
        "                    examples.append((f\"What is the trend of {value_col} over {time_col}?\", \"trend\"))\n",
        "                    examples.append((f\"How does {value_col} change over {time_col}?\", \"trend\"))\n",
        "                    examples.append((f\"Show me {value_col} over time using {time_col}.\", \"trend\"))\n",
        "\n",
        "        return examples\n",
        "\n",
        "    def preprocess_question(self, question):\n",
        "        \"\"\"Preprocess the question for better matching.\"\"\"\n",
        "        # Convert to lowercase\n",
        "        question = question.lower()\n",
        "\n",
        "        # Remove punctuation\n",
        "        question = question.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(question)\n",
        "\n",
        "        # Remove stop words\n",
        "        filtered_tokens = [word for word in tokens if word not in self.stop_words]\n",
        "\n",
        "        # Lemmatize words\n",
        "        lemmatized_tokens = [self.lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "        return ' '.join(lemmatized_tokens)\n",
        "\n",
        "    def extract_features(self, questions):\n",
        "        \"\"\"Extract features from a list of questions using TF-IDF.\"\"\"\n",
        "        # Preprocess questions\n",
        "        processed_questions = [self.preprocess_question(q) for q in questions]\n",
        "\n",
        "        # Create TF-IDF vectorizer\n",
        "        vectorizer = TfidfVectorizer(max_features=100)\n",
        "\n",
        "        # Fit and transform\n",
        "        features = vectorizer.fit_transform(processed_questions)\n",
        "\n",
        "        # Store vectorizer for future use\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "        return features\n",
        "\n",
        "    def train_question_classifier(self):\n",
        "        \"\"\"Train a classifier to identify question types.\"\"\"\n",
        "        if not self.example_questions:\n",
        "            print(\"No example questions available for training. Generate examples first.\")\n",
        "            return False\n",
        "\n",
        "        # Extract questions and labels\n",
        "        questions, labels = zip(*self.example_questions)\n",
        "\n",
        "        # Extract features\n",
        "        X = self.extract_features(questions)\n",
        "        y = labels\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Train classifier\n",
        "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        y_pred = clf.predict(X_test)\n",
        "        report = classification_report(y_test, y_pred)\n",
        "        print(\"Classification Report:\")\n",
        "        print(report)\n",
        "\n",
        "        # Store classifier\n",
        "        self.question_classifier = clf\n",
        "\n",
        "        return True\n",
        "\n",
        "    def identify_question_type(self, question):\n",
        "        \"\"\"Identify the type of question being asked.\"\"\"\n",
        "        # If we have a trained classifier, use it\n",
        "        if self.question_classifier is not None:\n",
        "            # Preprocess and extract features\n",
        "            processed_question = self.preprocess_question(question)\n",
        "            question_vector = self.vectorizer.transform([processed_question])\n",
        "\n",
        "            # Predict question type\n",
        "            question_type = self.question_classifier.predict(question_vector)[0]\n",
        "\n",
        "            # Calculate confidence score\n",
        "            confidence_scores = self.question_classifier.predict_proba(question_vector)[0]\n",
        "            max_score_idx = np.argmax(confidence_scores)\n",
        "            confidence = confidence_scores[max_score_idx]\n",
        "\n",
        "            # Fall back to similarity method if confidence is low\n",
        "            if confidence < 0.6:\n",
        "                return self.identify_question_type_by_similarity(question)\n",
        "\n",
        "            # Extract relevant columns mentioned in the question\n",
        "            mentioned_columns = []\n",
        "            for column in self.df.columns:\n",
        "                if column.lower() in question.lower():\n",
        "                    mentioned_columns.append(column)\n",
        "\n",
        "            # Try to identify conditions in filter-type questions\n",
        "            conditions = None\n",
        "            if \"where\" in question.lower() or \"when\" in question.lower():\n",
        "                # Simple condition extraction, can be enhanced further\n",
        "                condition_patterns = [\n",
        "                    r\"(greater|more|higher|larger|bigger) than (\\d+\\.?\\d*)\",\n",
        "                    r\"(less|lower|smaller) than (\\d+\\.?\\d*)\",\n",
        "                    r\"equals? (?:to )?(\\d+\\.?\\d*)\",\n",
        "                    r\"= ?(\\d+\\.?\\d*)\",\n",
        "                    r\"> ?(\\d+\\.?\\d*)\",\n",
        "                    r\"< ?(\\d+\\.?\\d*)\",\n",
        "                    r\"between (\\d+\\.?\\d*) and (\\d+\\.?\\d*)\"\n",
        "                ]\n",
        "\n",
        "                for pattern in condition_patterns:\n",
        "                    matches = re.findall(pattern, question.lower())\n",
        "                    if matches:\n",
        "                        conditions = matches\n",
        "                        break\n",
        "\n",
        "            # Return the identified question type and parameters\n",
        "            return {\n",
        "                'type': question_type,\n",
        "                'confidence': confidence,\n",
        "                'mentioned_columns': mentioned_columns,\n",
        "                'conditions': conditions,\n",
        "                'original_question': question,\n",
        "                'processed_question': processed_question\n",
        "            }\n",
        "        else:\n",
        "            # Fall back to similarity method if no classifier is trained\n",
        "            return self.identify_question_type_by_similarity(question)\n",
        "\n",
        "    def identify_question_type_by_similarity(self, question):\n",
        "        \"\"\"Identify question type using similarity to templates.\"\"\"\n",
        "        # Preprocess the question\n",
        "        processed_question = self.preprocess_question(question)\n",
        "\n",
        "        # Convert templates to processed form for comparison\n",
        "        processed_templates = {k: self.preprocess_question(v) for k, v in self.qa_templates.items()}\n",
        "\n",
        "        # Calculate similarity between question and templates\n",
        "        vectorizer = TfidfVectorizer()\n",
        "\n",
        "        # Create a combined list of the processed question and all processed templates\n",
        "        all_texts = [processed_question] + list(processed_templates.values())\n",
        "\n",
        "        # Fit the vectorizer to all texts\n",
        "        tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "        # Calculate similarity between the question and each template\n",
        "        # Complete the identify_question_type_by_similarity method\n",
        "    def identify_question_type_by_similarity(self, question):\n",
        "        \"\"\"Identify question type using similarity to templates.\"\"\"\n",
        "        # Preprocess the question\n",
        "        processed_question = self.preprocess_question(question)\n",
        "\n",
        "        # Convert templates to processed form for comparison\n",
        "        processed_templates = {k: self.preprocess_question(v) for k, v in self.qa_templates.items()}\n",
        "\n",
        "        # Calculate similarity between question and templates\n",
        "        vectorizer = TfidfVectorizer()\n",
        "\n",
        "        # Create a combined list of the processed question and all processed templates\n",
        "        all_texts = [processed_question] + list(processed_templates.values())\n",
        "\n",
        "        # Fit the vectorizer to all texts\n",
        "        tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "        # Calculate similarity between the question and each template\n",
        "        similarities = {}\n",
        "        for i, (q_type, _) in enumerate(processed_templates.items()):\n",
        "            # Add 1 to i because the processed question is at index 0\n",
        "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[i+1:i+2])[0][0]\n",
        "            similarities[q_type] = similarity\n",
        "\n",
        "        # Find the most similar template\n",
        "        question_type = max(similarities, key=similarities.get)\n",
        "        confidence = similarities[question_type]\n",
        "\n",
        "        # Extract mentioned columns\n",
        "        mentioned_columns = []\n",
        "        for column in self.df.columns:\n",
        "            if column.lower() in question.lower():\n",
        "                mentioned_columns.append(column)\n",
        "\n",
        "        # Try to identify numeric values for comparisons\n",
        "        numbers = re.findall(r'\\b\\d+\\.?\\d*\\b', question)\n",
        "\n",
        "        # Return the identified question type and parameters\n",
        "        return {\n",
        "            'type': question_type,\n",
        "            'confidence': confidence,\n",
        "            'mentioned_columns': mentioned_columns,\n",
        "            'numbers': numbers if numbers else None,\n",
        "            'original_question': question,\n",
        "            'processed_question': processed_question\n",
        "        }\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        \"\"\"Process a question and return the answer based on the data.\"\"\"\n",
        "        # First, identify the question type\n",
        "        question_info = self.identify_question_type(question)\n",
        "\n",
        "        # Get the column(s) mentioned in the question\n",
        "        columns = question_info.get('mentioned_columns', [])\n",
        "\n",
        "        # If no columns were explicitly mentioned but the question requires a column,\n",
        "        # try to infer a relevant column based on the question type\n",
        "        if not columns and question_info['type'] not in ['count']:\n",
        "            # For questions about trends, prefer datetime columns\n",
        "            if question_info['type'] == 'trend':\n",
        "                for col, desc in self.column_descriptions.items():\n",
        "                    if desc.get('semantic_type') == 'date/time':\n",
        "                        columns.append(col)\n",
        "                        break\n",
        "            # For questions about averages, sums, etc., prefer numeric columns\n",
        "            elif question_info['type'] in ['min', 'max', 'mean', 'sum', 'top', 'bottom', 'distribution']:\n",
        "                for col, desc in self.column_descriptions.items():\n",
        "                    if desc.get('type') == 'numeric':\n",
        "                        columns.append(col)\n",
        "                        break\n",
        "\n",
        "        # Process the question based on its type\n",
        "        try:\n",
        "            if question_info['type'] == 'count':\n",
        "                if 'column' in question.lower() and columns:\n",
        "                    # Count unique values in the specified column\n",
        "                    unique_count = self.df[columns[0]].nunique()\n",
        "                    answer = f\"There are {unique_count} unique values in the '{columns[0]}' column.\"\n",
        "                else:\n",
        "                    # Count rows in the dataset\n",
        "                    answer = f\"There are {self.data_summary['total_rows']} rows in the dataset.\"\n",
        "\n",
        "            elif question_info['type'] == 'min' and columns:\n",
        "                # Find minimum value\n",
        "                min_val = self.df[columns[0]].min()\n",
        "                answer = f\"The minimum value of '{columns[0]}' is {min_val}.\"\n",
        "\n",
        "            elif question_info['type'] == 'max' and columns:\n",
        "                # Find maximum value\n",
        "                max_val = self.df[columns[0]].max()\n",
        "                answer = f\"The maximum value of '{columns[0]}' is {max_val}.\"\n",
        "\n",
        "            elif question_info['type'] == 'mean' and columns:\n",
        "                # Calculate mean\n",
        "                mean_val = self.df[columns[0]].mean()\n",
        "                answer = f\"The average/mean value of '{columns[0]}' is {mean_val:.2f}.\"\n",
        "\n",
        "            elif question_info['type'] == 'sum' and columns:\n",
        "                # Calculate sum\n",
        "                sum_val = self.df[columns[0]].sum()\n",
        "                answer = f\"The sum of '{columns[0]}' is {sum_val}.\"\n",
        "\n",
        "            elif question_info['type'] == 'unique' and columns:\n",
        "                # Get unique values\n",
        "                unique_vals = self.df[columns[0]].unique()\n",
        "                if len(unique_vals) > 10:\n",
        "                    # If there are many unique values, just show the count\n",
        "                    answer = f\"There are {len(unique_vals)} unique values in '{columns[0]}'.\"\n",
        "                else:\n",
        "                    # Otherwise, list all unique values\n",
        "                    answer = f\"The unique values in '{columns[0]}' are: {', '.join(map(str, unique_vals))}.\"\n",
        "\n",
        "            elif question_info['type'] == 'missing' and columns:\n",
        "                # Count missing values\n",
        "                missing_count = self.df[columns[0]].isna().sum()\n",
        "                answer = f\"There are {missing_count} missing values in the '{columns[0]}' column.\"\n",
        "\n",
        "            elif question_info['type'] == 'correlation' and len(columns) >= 2:\n",
        "                # Calculate correlation between two columns\n",
        "                corr = self.df[columns[0]].corr(self.df[columns[1]])\n",
        "                strength = \"strong positive\" if corr > 0.7 else \"moderate positive\" if corr > 0.3 else \"weak positive\" if corr > 0 else \"no\" if corr == 0 else \"weak negative\" if corr > -0.3 else \"moderate negative\" if corr > -0.7 else \"strong negative\"\n",
        "                answer = f\"The correlation between '{columns[0]}' and '{columns[1]}' is {corr:.2f}, which indicates a {strength} correlation.\"\n",
        "\n",
        "            elif question_info['type'] == 'filter':\n",
        "                # Extract condition parts\n",
        "                if question_info.get('conditions'):\n",
        "                    # Process complex conditions\n",
        "                    filter_expr = \"\"\n",
        "                    for condition in question_info['conditions']:\n",
        "                        if isinstance(condition, tuple):\n",
        "                            if condition[0].startswith(('greater', 'more', 'higher', 'larger', 'bigger')):\n",
        "                                filter_expr = f\"{columns[0]} > {float(condition[1])}\"\n",
        "                            elif condition[0].startswith(('less', 'lower', 'smaller')):\n",
        "                                filter_expr = f\"{columns[0]} < {float(condition[1])}\"\n",
        "                            elif 'equal' in condition[0] or '=' in condition[0]:\n",
        "                                filter_expr = f\"{columns[0]} == {float(condition[1])}\"\n",
        "                            elif 'between' in condition[0] and len(condition) >= 2:\n",
        "                                filter_expr = f\"{columns[0]} > {float(condition[0])} and {columns[0]} < {float(condition[1])}\"\n",
        "                else:\n",
        "                    # Try to infer condition from question\n",
        "                    numbers = question_info.get('numbers', [])\n",
        "                    if numbers and '>' in question:\n",
        "                        filter_expr = f\"{columns[0]} > {float(numbers[0])}\"\n",
        "                    elif numbers and '<' in question:\n",
        "                        filter_expr = f\"{columns[0]} < {float(numbers[0])}\"\n",
        "                    elif numbers and ('=' in question or 'equal' in question.lower()):\n",
        "                        filter_expr = f\"{columns[0]} == {float(numbers[0])}\"\n",
        "                    else:\n",
        "                        # Default case\n",
        "                        filter_expr = f\"{columns[0]} > {self.df[columns[0]].mean()}\"\n",
        "\n",
        "                # Apply the filter\n",
        "                filtered_df = self.df.query(filter_expr) if filter_expr else self.df\n",
        "                if len(filtered_df) > 5:\n",
        "                    answer = f\"Found {len(filtered_df)} rows matching the condition. Here are the first 5:\\n{filtered_df.head(5).to_string()}\"\n",
        "                else:\n",
        "                    answer = f\"Found {len(filtered_df)} rows matching the condition:\\n{filtered_df.to_string()}\"\n",
        "\n",
        "            elif question_info['type'] == 'group' and len(columns) >= 2:\n",
        "                # Determine which column to group by and which to aggregate\n",
        "                numeric_col = None\n",
        "                group_col = None\n",
        "\n",
        "                for col in columns:\n",
        "                    if pd.api.types.is_numeric_dtype(self.df[col].dtype):\n",
        "                        numeric_col = col\n",
        "                    else:\n",
        "                        group_col = col\n",
        "\n",
        "                # If we couldn't determine, assume first is group, second is value\n",
        "                if numeric_col is None or group_col is None:\n",
        "                    group_col = columns[0]\n",
        "                    numeric_col = columns[1]\n",
        "\n",
        "                # Perform groupby and aggregation\n",
        "                result = self.df.groupby(group_col)[numeric_col].agg(['mean', 'sum', 'count']).reset_index()\n",
        "                answer = f\"Grouped by '{group_col}':\\n{result.to_string()}\"\n",
        "\n",
        "            elif question_info['type'] == 'top' and columns:\n",
        "                # Find top N values\n",
        "                # Try to extract N from the question\n",
        "                n = 5  # Default\n",
        "                for num in question_info.get('numbers', []):\n",
        "                    if int(float(num)) > 0:\n",
        "                        n = int(float(num))\n",
        "                        break\n",
        "\n",
        "                top_values = self.df.nlargest(n, columns[0])\n",
        "                answer = f\"Top {n} values in '{columns[0]}':\\n{top_values.to_string()}\"\n",
        "\n",
        "            elif question_info['type'] == 'bottom' and columns:\n",
        "                # Find bottom N values\n",
        "                # Try to extract N from the question\n",
        "                n = 5  # Default\n",
        "                for num in question_info.get('numbers', []):\n",
        "                    if int(float(num)) > 0:\n",
        "                        n = int(float(num))\n",
        "                        break\n",
        "\n",
        "                bottom_values = self.df.nsmallest(n, columns[0])\n",
        "                answer = f\"Bottom {n} values in '{columns[0]}':\\n{bottom_values.to_string()}\"\n",
        "\n",
        "            elif question_info['type'] == 'distribution' and columns:\n",
        "                # Describe the distribution\n",
        "                desc = self.df[columns[0]].describe()\n",
        "                answer = f\"Distribution of '{columns[0]}':\\n{desc.to_string()}\"\n",
        "\n",
        "                # Add visualization hint\n",
        "                answer += \"\\n\\nTo visualize this distribution, you can use:\"\n",
        "                answer += f\"\\n\\nplt.figure(figsize=(10, 6))\\nsns.histplot(data=df, x='{columns[0]}')\\nplt.title('Distribution of {columns[0]}')\\nplt.show()\"\n",
        "\n",
        "            elif question_info['type'] == 'trend' and len(columns) >= 2:\n",
        "                # Determine which column is time and which is value\n",
        "                time_col = None\n",
        "                value_col = None\n",
        "\n",
        "                for col in columns:\n",
        "                    if self.column_descriptions[col].get('semantic_type') == 'date/time':\n",
        "                        time_col = col\n",
        "                    elif pd.api.types.is_numeric_dtype(self.df[col].dtype):\n",
        "                        value_col = col\n",
        "\n",
        "                # If we couldn't determine, assume first is time, second is value\n",
        "                if time_col is None or value_col is None:\n",
        "                    time_col = columns[0]\n",
        "                    value_col = columns[1]\n",
        "\n",
        "                # Create a simple description of the trend\n",
        "                # Sort by time column\n",
        "                trend_data = self.df.sort_values(time_col)\n",
        "                first_value = trend_data[value_col].iloc[0]\n",
        "                last_value = trend_data[value_col].iloc[-1]\n",
        "                change = ((last_value - first_value) / first_value) * 100 if first_value != 0 else 0\n",
        "\n",
        "                trend_direction = \"increasing\" if change > 5 else \"decreasing\" if change < -5 else \"stable\"\n",
        "\n",
        "                answer = f\"The trend of '{value_col}' over '{time_col}' is {trend_direction}. \"\n",
        "                answer += f\"From {trend_data[time_col].iloc[0]} to {trend_data[time_col].iloc[-1]}, \"\n",
        "                answer += f\"the value changed from {first_value:.2f} to {last_value:.2f} ({change:.1f}% change).\"\n",
        "\n",
        "                # Add visualization hint\n",
        "                answer += \"\\n\\nTo visualize this trend, you can use:\"\n",
        "                answer += f\"\\n\\nplt.figure(figsize=(12, 6))\\nplt.plot(df['{time_col}'], df['{value_col}'])\\nplt.title('Trend of {value_col} over {time_col}')\\nplt.xlabel('{time_col}')\\nplt.ylabel('{value_col}')\\nplt.grid(True)\\nplt.show()\"\n",
        "\n",
        "            else:\n",
        "                # Generic response for unrecognized question type\n",
        "                answer = f\"I couldn't fully understand your question about the data. Here's some basic information about the dataset:\\n\"\n",
        "                answer += f\"- Dataset has {self.data_summary['total_rows']} rows and {self.data_summary['total_columns']} columns\\n\"\n",
        "                answer += f\"- Columns: {', '.join(self.data_summary['column_names'])}\\n\"\n",
        "                answer += \"Please try asking a more specific question about the data.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle any errors\n",
        "            answer = f\"Error processing your question: {str(e)}\\n\"\n",
        "            answer += \"Please try rephrasing your question or specify the column names more clearly.\"\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def interactive_qa(self):\n",
        "        \"\"\"Start an interactive Q&A session with the user.\"\"\"\n",
        "        # Train the question classifier if not already trained\n",
        "        if self.question_classifier is None:\n",
        "            print(\"Training question classifier...\")\n",
        "            self.train_question_classifier()\n",
        "\n",
        "        print(\"\\n===== Excel Question Answering System =====\")\n",
        "        print(f\"Loaded file: {self.file_path}\")\n",
        "        print(f\"Dataset has {self.data_summary['total_rows']} rows and {self.data_summary['total_columns']} columns\")\n",
        "        print(\"Columns:\", \", \".join(self.data_summary['column_names']))\n",
        "        print(\"\\nAsk questions about your data (type 'exit' to quit):\")\n",
        "\n",
        "        while True:\n",
        "            question = input(\"\\nYour question: \")\n",
        "\n",
        "            if question.lower() in ['exit', 'quit', 'bye']:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "\n",
        "            # Process the question and get the answer\n",
        "            answer = self.answer_question(question)\n",
        "\n",
        "            # Print the answer\n",
        "            print(\"\\nAnswer:\")\n",
        "            print(answer)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your Excel file path\n",
        "    file_path = \"/content/Online Retail.xlsx\"\n",
        "\n",
        "    # Create the QA system\n",
        "    qa_system =ExcelQuestionAnswerer(file_path)\n",
        "\n",
        "    # Start interactive session\n",
        "    qa_system.interactive_qa()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Nsp8GIqsi6Zt"
      }
    }
  ]
}